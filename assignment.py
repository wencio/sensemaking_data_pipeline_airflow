from airflow import DAG  # Import the DAG class from the Airflow library
from datetime import timedelta  # Import timedelta for defining time intervals
from airflow.operators.bash import BashOperator  # Import BashOperator for running bash commands
from airflow.utils.dates import days_ago  # Import days_ago for setting start dates
from airflow.operators.python import PythonOperator  # Import PythonOperator for running Python functions
import glob  # Import glob for file handling
import json  # Import the JSON library for working with JSON data
import requests  # Import the requests library for making HTTP requests
import time  # Import the time library for adding delays
from collections import Counter  # Import the Counter class for counting elements

def catalog():
    # Define pull(url) helper function
    def pull(url):
        # Send a GET request to the specified URL
        response = requests.get(url)
        # Extract the text content from the response
        data = response.text
        # Return the data
        return data

    # Define store(data, file) helper function
    def store(data, file):
        # Create and open a file with the specified filename in write mode
        full_path = f"/opt/airflow/dags/{file}"
        print(f"Storing data at: {full_path}")
        with open(full_path, 'w', encoding='utf-8') as f:
            # Write the data to the file
            f.write(data)
        # Print a message indicating the file has been written
        print('wrote file: ' + file)

    # Read URLs from 00_urls.txt
    file_path = '/opt/airflow/dags/00_urls.txt'
    with open(file_path, 'r') as f:
        # Read all lines from the file, strip whitespace, and store in a list
        urls = [line.strip() for line in f.readlines()]

    # Iterate through the URLs
    for url in urls:
        # Call pull function to get the data from the URL
        data = pull(url)

        # Extract the file name from the URL
        index = url.rfind('/') + 1
        file = url[index:]

        # Call store function to save the data to a file
        store(data, file)

        # Print a message indicating the data has been pulled
        print('pulled: ' + file)
        # Print a message indicating a wait time
        print('--- waiting ---')
        # Wait for 15 seconds before processing the next URL
        time.sleep(15)

# Function: Catalog
# This function reads a list of URLs from a file, fetches the HTML content of each URL, and saves it to individual files in the specified directory.

def combine():
    # Open the output file in write mode
    output_path = '/opt/airflow/dags/combo.txt'
    with open(output_path, 'w', encoding='utf-8') as outfile:
        # Iterate over all .html files in the current directory
        for file in glob.glob("*.html"):
            # Open each file in read mode
            with open(file, 'r', encoding='utf-8') as infile:
                # Write the contents of the file to the output file
                outfile.write(infile.read())
                # Optionally, add a newline or some separator between file contents
                outfile.write('\n')  # Adding a newline between file contents for clarity

    print('Combined all .html files into combo.txt')

# Function: Combine
# This function reads all .html files in the specified directory, combines their content into a single file, and saves it as combo.txt.

def titles():
    # Define the store_json helper function
    def store_json(data, file):
        # Open the specified file in write mode with UTF-8 encoding
        full_path = f"/opt/airflow/dags/{file}"
        with open(full_path, 'w', encoding='utf-8') as f:
            # Dump the data into the file in JSON format, ensuring ASCII characters are preserved and the output is pretty-printed
            json.dump(data, f, ensure_ascii=False, indent=4)
            # Print a message indicating the file has been written
            print('wrote file: ' + file)

    # Open and read the large HTML file generated by combine()
    with open('/opt/airflow/dags/combo.txt', 'r', encoding='utf-8') as f:
        # Read the entire content of the file into the html variable
        html = f.read()

    # Replace new line and carriage return characters with spaces
    html = html.replace('\n', ' ').replace('\r', '')

    # Create a BeautifulSoup object to parse the HTML content
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html, "html.parser")

    # Find all h3 elements in the HTML content
    results = soup.find_all('h3')

    # Extract the text from each h3 element and store it in a list
    titles = [item.text for item in results]

    # Store the results in a JSON file
    store_json(titles, 'titles.json')

# Function: Titles
# This function reads the combined HTML file, extracts the text from all <h3> elements, and saves the extracted titles into a JSON file.

def clean():
    # Define the store_json helper function
    def store_json(data, file):
        # Open the specified file in write mode with UTF-8 encoding
        full_path = f"/opt/airflow/dags/{file}"
        with open(full_path, 'w', encoding='utf-8') as f:
            # Dump the data into the file in JSON format, ensuring ASCII characters are preserved and the output is pretty-printed
            json.dump(data, f, ensure_ascii=False, indent=4)
            # Print a message indicating the file has been written
            print('wrote file: ' + file)

    # Open and read the titles.json file
    with open('/opt/airflow/dags/titles.json', 'r', encoding='utf-8') as file:
        # Load the JSON content from the file into the titles variable
        titles = json.load(file)

        # Remove punctuation and numbers from each title
        for index, title in enumerate(titles):
            # Define the string of punctuation and numbers to remove
            punctuation = '''!()-[]{};:'"\,<>./?@#$%^&*_~1234567890'''
            # Create a translation table mapping each character in punctuation to None
            translationTable = str.maketrans("", "", punctuation)
            # Remove the punctuation and numbers from the title using the translation table
            clean = title.translate(translationTable)
            # Update the title in the titles list with the cleaned title
            titles[index] = clean

        # Remove one-character words from each title
        for index, title in enumerate(titles):
            # Split the title into words, filter out one-character words, and join them back into a cleaned title
            clean = ' '.join([word for word in title.split() if len(word) > 1])
            # Update the title in the titles list with the cleaned title
            titles[index] = clean

        # Store the cleaned titles in a new JSON file
        store_json(titles, 'titles_clean.json')

# Function: Clean
# This function reads the titles from the JSON file, removes punctuation and numbers, filters out one-character words, and saves the cleaned titles into a new JSON file.

def count_words():
    # Define the store_json helper function
    def store_json(data, file):
        full_path = f"/opt/airflow/dags/{file}"
        with open(full_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print('wrote file: ' + file)

    # Open and read the titles_clean.json file
    with open('/opt/airflow/dags/titles_clean.json', 'r', encoding='utf-8') as file:
        titles = json.load(file)
        words = []

        # Extract words and flatten the list
        for title in titles:
            words.extend(title.split())

        # Count word frequency
        counts = Counter(words)

        # Store the word counts in a JSON file
        store_json(counts, 'words.json')

# Function: Count Words
# This function reads the cleaned titles from the JSON file, counts the frequency of each word, and saves the word counts into a new JSON file.

# Define the DAG and tasks
with DAG(
    "assignment",  # Define the DAG name
    start_date=days_ago(1),  # Set the start date for the DAG
    schedule_interval="@daily",  # Set the schedule interval for the DAG
    catchup=False,  # Disable backfilling
) as dag:

    t0 = BashOperator(
        task_id='task_zero',  # Define the task ID
        bash_command='pip install beautifulsoup4',  # Install BeautifulSoup4 using pip
        retries=2  # Set the number of retries
    )

    t1 = PythonOperator(
        task_id='task_one',  # Define the task ID
        depends_on_past=False,  # Do not depend on past runs
        python_callable=catalog  # Call the catalog function
    )

    t2 = PythonOperator(
        task_id='task_two',  # Define the task ID
        depends_on_past=False,  # Do not depend on past runs
        python_callable=combine  # Call the combine function

         )

    t3 = PythonOperator(
        task_id='task_three',
        depends_on_past=False,
        python_callable=titles # Call the title function
    )

    t4 = PythonOperator(
        task_id='task_four',
        depends_on_past=False,
        python_callable=clean # Call the clean function
    )

    t5 = PythonOperator(
        task_id='task_five',
        depends_on_past=False,
        python_callable=count_words # Call count_words function
    )

    t0 >> t1 >> t2 >> t3 >> t4 >> t5

